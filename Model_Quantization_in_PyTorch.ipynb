{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgbpNhKbr6h1"
      },
      "source": [
        "#### What is Quantization?\n",
        "Quantization is a core method used to reduce computational and memory costs of large neural network models by converting high precision floating point representations (e.g. float32) to lower precision integer types (e.g. int8) with minimal impact on model accuracy. This process helps to significantly reduce inference time and compute resources.\n",
        "\n",
        "Quantization reduces neural network weights and activations from 32-bit floats to 8-bit integers, reducing storage and memory requirements by up to 4x. Int8 matrix multiplication is also much faster on most hardwares (e.g. cpu and embedded accelerators)\n",
        "\n",
        "#### How Quantization works?\n",
        "Quantization replaces original Weighs $W$ and biases $b$ stored as 32-bit floats with a lower-precision integer representation usually $W$ represented in int8 while $b$ in int32. The matrix multiplication with the input to the layer is then performed in this lower precision representation before de-quantization via the scale and zero-point parameters and feeding the output to the next layer.\n",
        "\n",
        "<!-- #### Types of Quantization -->\n",
        "\n",
        "#### PyTorch Implementation of Post-Training Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weOFG4QojJs2",
        "outputId": "ab90195a-af0f-4578-9f5e-17c59a8d3c31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.56MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.26MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.60MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (quant): QuantStub()\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=12544, out_features=128, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor()])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# The Model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # Add a quantization layer\n",
        "        self.quant = torch.quantization.QuantStub()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        # self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.maxpool2 = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(14 * 14 * 64, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        # Add dequantization layer\n",
        "        self.dequant = torch.quantization.DeQuantStub()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.quant(x) # quantize input to int8\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dequant(x) # dequantize output to float\n",
        "        return x\n",
        "\n",
        "model_fp32 = CNN()\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihjeyqE6xml3",
        "outputId": "36c4801b-c7ae-4755-9b23-257156153273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda\n",
            "Epoch 1, Loss: 0.01163397313121483\n",
            "Epoch 2, Loss: 0.00725513479790467\n",
            "Epoch 3, Loss: 0.007493622528057428\n",
            "Epoch 4, Loss: 0.005139252133150162\n",
            "Epoch 5, Loss: 0.004771352358855667\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "import torch.optim as optim\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_fp32.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_fp32.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"Training on {device}\")\n",
        "for epoch in range(5):\n",
        "    running_loss = 0.0\n",
        "    model_fp32.train()\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_fp32(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HjECoVtqzUfI"
      },
      "outputs": [],
      "source": [
        "# Fuse Layers\n",
        "def fuse_model(model):\n",
        "    torch.quantization.fuse_modules(model_fp32,\n",
        "     [['conv1', 'relu1'], ['conv2', 'relu2'], ['fc1', 'relu3']], inplace=True)\n",
        "fuse_model(model_fp32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NHyBclJL7zYE"
      },
      "outputs": [],
      "source": [
        "# Set quantization configuration. We use the fbgemm configuration\n",
        "# FBGEMM (Facebook GEneral Matrix Multiplication) is a low-precision,\n",
        "# high-performance matrix-matrix multiplications and convolution library for\n",
        "# server-side inference. This is preferred for x86 CPUs. For ARM CPUs use 'qnnpack'.\n",
        "model_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4BIngKE7zbN",
        "outputId": "57fde9cc-a095-4b49-c930-0f973289505c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1592661657.py:3: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  model_fp32_prepared = torch.quantization.prepare(model_fp32)\n"
          ]
        }
      ],
      "source": [
        "# Prepare for quantization\n",
        "model_fp32.cpu()\n",
        "model_fp32_prepared = torch.quantization.prepare(model_fp32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhAlYiz17zlZ",
        "outputId": "9be4eda6-8f28-47c6-cf0d-006e1138b850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0079]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.0)\n",
              "    )\n",
              "  )\n",
              "  (conv1): ConvReLU2d(\n",
              "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0090]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=1.1397202014923096)\n",
              "    )\n",
              "  )\n",
              "  (relu1): Identity()\n",
              "  (conv2): ConvReLU2d(\n",
              "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0449]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=5.702958106994629)\n",
              "    )\n",
              "  )\n",
              "  (relu2): Identity()\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): LinearReLU(\n",
              "    (0): Linear(in_features=12544, out_features=128, bias=True)\n",
              "    (1): ReLU()\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2991]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=0.0, max_val=37.985416412353516)\n",
              "    )\n",
              "  )\n",
              "  (relu3): Identity()\n",
              "  (fc2): Linear(\n",
              "    in_features=128, out_features=10, bias=True\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.5427]), zero_point=tensor([59], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-31.943708419799805, max_val=36.98019027709961)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_fp32_prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJx3xUuz7zel",
        "outputId": "19ad0432-5c89-46dd-ccc1-ed992e6ee019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibrating...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calibration with a sample data\n",
        "print(\"Calibrating...\\n\")\n",
        "model_fp32_prepared.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in train_loader:\n",
        "        model_fp32_prepared(images)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ92DhPI7zh1",
        "outputId": "ed74cd1d-d273-49e2-9543-5f7f087c3809"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2579158373.py:2: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = torch.quantization.convert(model_fp32_prepared, inplace=False)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (quant): Quantize(scale=tensor([0.0079]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "  (conv1): QuantizedConvReLU2d(1, 32, kernel_size=(3, 3), stride=(1, 1), scale=0.008974174968898296, zero_point=0, padding=(1, 1))\n",
              "  (relu1): Identity()\n",
              "  (conv2): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04490518197417259, zero_point=0, padding=(1, 1))\n",
              "  (relu2): Identity()\n",
              "  (maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): QuantizedLinearReLU(in_features=12544, out_features=128, scale=0.29909777641296387, zero_point=0, qscheme=torch.per_channel_affine)\n",
              "  (relu3): Identity()\n",
              "  (fc2): QuantizedLinear(in_features=128, out_features=10, scale=0.5427078604698181, zero_point=59, qscheme=torch.per_channel_affine)\n",
              "  (dequant): DeQuantize()\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert to Quantized Model\n",
        "quantized_model = torch.quantization.convert(model_fp32_prepared, inplace=False)\n",
        "quantized_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWBsUaesAgPr",
        "outputId": "d80a58f8-ff7a-4a78-ec95-58484234441c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating original float32 model\n",
            "Accuracy: 98.89%\n",
            "Evaluating quantized model\n",
            "Accuracy: 98.91%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.9891"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run the test data through each model and compare float32 and quantized int8 accuracy\n",
        "def evaluate_model(model, data_loader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Accuracy: {100.0 * correct / total}%\")\n",
        "    return correct / total\n",
        "\n",
        "print(f\"Evaluating original float32 model\")\n",
        "evaluate_model(model_fp32, test_loader)\n",
        "\n",
        "print(f\"Evaluating quantized model\")\n",
        "evaluate_model(quantized_model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwJXEcnuBWy9",
        "outputId": "64821055-f9f9-44ca-d493-557b1ced525c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Model Size: 6.205862998962402 MB\n",
            "Quantized Model Size: 1.5630693435668945 MB\n"
          ]
        }
      ],
      "source": [
        "# Let's check the model file sizes\n",
        "import os\n",
        "\n",
        "# Save model\n",
        "torch.save(model_fp32.state_dict(), \"fp32_model.pth\")\n",
        "torch.save(quantized_model.state_dict(), \"quantized_model.pth\")\n",
        "\n",
        "fp32_size = os.path.getsize(\"fp32_model.pth\")\n",
        "quantized_size = os.path.getsize(\"quantized_model.pth\")\n",
        "\n",
        "print(f\"Original Model Size: {fp32_size / (1024 * 1024)} MB\")\n",
        "print(f\"Quantized Model Size: {quantized_size / (1024 * 1024)} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdY7yhjyDvmq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
